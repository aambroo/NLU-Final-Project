{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Estimator Fine Tuning\n",
    "In this notebook we will cover two stages that are essential for a proper training. \\\n",
    "These will be **data preprocessing** and **vectorizer**/**classificator finetuning**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Retrieval & Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load Datasets\n",
    "- Load Rotten IMDB Dataset to train subjectivity detector.\n",
    "- Load Movie Reviews Dataset to extract its vocabulary.\n",
    "  \n",
    "*note: as it's carried out in the original paper*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T18:18:39.637896Z",
     "iopub.status.busy": "2022-08-04T18:18:39.637379Z",
     "iopub.status.idle": "2022-08-04T18:18:40.025853Z",
     "shell.execute_reply": "2022-08-04T18:18:40.025563Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import nltk\n",
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "\n",
    "def load_rotten_imdb(path):      #---> utils.preprocessing\n",
    "    subjective_sentences = \"quote.tok.gt9.5000\"\n",
    "    objective_sentences = \"plot.tok.gt9.5000\"\n",
    "\n",
    "    subj = []\n",
    "    with open(os.path.join(path, subjective_sentences), 'r') as f:\n",
    "        [subj.append(sent.strip()) for sent in f.readlines()]\n",
    "\n",
    "    obj = []\n",
    "    with open(os.path.join(path, objective_sentences), 'r') as f:\n",
    "        [obj.append(sent.strip()) for sent in f.readlines()]\n",
    "\n",
    "    return subj, obj\n",
    "\n",
    "\n",
    "def lol2str(doc):       #---> utils.preprocessing\n",
    "    \"\"\"Transforms a document in the list-of-lists format into\n",
    "    a block of text (str type).\"\"\"\n",
    "    return \" \".join([word for sent in doc for word in sent])\n",
    "\n",
    "\n",
    "def mr2str(dataset):    #---> utils.preprocessing\n",
    "    \"\"\"Transforms the Movie Reviews Dataset (or a slice) into a block of text.\"\"\"\n",
    "    return [lol2str(doc) for doc in dataset]\n",
    "\n",
    "\n",
    "def get_movie_reviews_dataset(mark_negs:bool = True) -> str:    #---> utils.preprocessing\n",
    "    \"\"\"Uses the nltk library to download the \"Movie Reviews\" dateset,\n",
    "    splitting it into negative reviews and positive reviews.\n",
    "    Toggle :param mark_neg: if u wish sentences to be mark-negated or not.\"\"\"\n",
    "    nltk.download(\"movie_reviews\")\n",
    "    from nltk.corpus import movie_reviews\n",
    "    neg = movie_reviews.paras(categories=\"neg\")\n",
    "    pos = movie_reviews.paras(categories=\"pos\")\n",
    "    if mark_negs:\n",
    "        neg = [[mark_negation(sent) for sent in doc] for doc in neg]\n",
    "        pos = [[mark_negation(sent) for sent in doc] for doc in pos]\n",
    "    return pos, neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T18:18:40.027659Z",
     "iopub.status.busy": "2022-08-04T18:18:40.027519Z",
     "iopub.status.idle": "2022-08-04T18:18:40.031574Z",
     "shell.execute_reply": "2022-08-04T18:18:40.031332Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_TO_IMDB = 'data/rotten_imdb/'\n",
    "\n",
    "subj, obj = load_rotten_imdb(path=PATH_TO_IMDB)\n",
    "imdb_ds = subj + obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T18:18:40.033080Z",
     "iopub.status.busy": "2022-08-04T18:18:40.032996Z",
     "iopub.status.idle": "2022-08-04T18:18:42.466308Z",
     "shell.execute_reply": "2022-08-04T18:18:42.465968Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/matteoambrosini/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_MOVIE_REVIEWS = 'data/movie_reviews/'\n",
    "\n",
    "pos, neg = get_movie_reviews_dataset(mark_negs=True)\n",
    "pos = mr2str(pos)\n",
    "neg = mr2str(neg)\n",
    "movie_reviews_ds = pos + neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Define DiffPosNeg as in the original paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T18:18:42.468347Z",
     "iopub.status.busy": "2022-08-04T18:18:42.468234Z",
     "iopub.status.idle": "2022-08-04T18:18:42.482532Z",
     "shell.execute_reply": "2022-08-04T18:18:42.482259Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def hconcat(X1: np.ndarray, X2: np.ndarray) -> np.ndarray:      #---> utils.preprocessing\n",
    "    \"\"\"Applies horizontal concatenation to the X1 and X2 matrices, returning the concatenated matrix.\"\"\"\n",
    "    assert len(X1.shape) == len(\n",
    "        X2.shape) == 2, \"function 'hconcat' only works with matrices (np.array with 2 dimensions).\"\n",
    "    assert X1.shape[0] == X2.shape[0], \"In order to hconcat matrices, they must have the same number of rows.\"\n",
    "    N = X1.shape[0]\n",
    "    M = X1.shape[1] + X2.shape[1]\n",
    "    X = np.ndarray(shape=(N, M))\n",
    "    X[:, :X1.shape[1]] = X1\n",
    "    X[:, X1.shape[1]:] = X2\n",
    "    return X\n",
    "\n",
    "\n",
    "################################\n",
    "########## DIFFPOSNEG ##########\n",
    "################################\n",
    "# Imports:\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet\n",
    "import multiprocessing as mp\n",
    "import time\n",
    "\n",
    "\n",
    "pos2wn = {\"NOUN\": \"n\", \"VERB\": \"v\", \"ADJ\": \"a\", \"ADV\": \"r\"}\n",
    "\n",
    "\n",
    "def lesk(context_sentence, ambiguous_word, pos=None, synsets=None):\n",
    "    \"\"\"Return a synset for an ambiguous word in a context.\n",
    "\n",
    "    :param iter context_sentence: The context sentence where the ambiguous word\n",
    "         occurs, passed as an iterable of words.\n",
    "    :param str ambiguous_word: The ambiguous word that requires WSD.\n",
    "    :param str pos: A specified Part-of-Speech (POS).\n",
    "    :param iter synsets: Possible synsets of the ambiguous word.\n",
    "    :return: ``lesk_sense`` The Synset() object with the highest signature overlaps.\n",
    "    \"\"\"\n",
    "\n",
    "    context = set(context_sentence)\n",
    "    if synsets is None:\n",
    "        synsets = wordnet.synsets(ambiguous_word)\n",
    "\n",
    "    if pos:\n",
    "        if pos == 'a':\n",
    "            synsets = [ss for ss in synsets if str(ss.pos()) in ['a', 's']]\n",
    "        else:\n",
    "            synsets = [ss for ss in synsets if str(ss.pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "\n",
    "    _, sense = max(\n",
    "        (len(context.intersection(ss.definition().split())), ss) for ss in synsets\n",
    "    )\n",
    "\n",
    "    return sense\n",
    "\n",
    "\n",
    "def valence_count(sent, tokenizer, memory, update_mem):\n",
    "    \"\"\"Given a string :param: sent, returns the count of both\n",
    "    positive and negative tokens in it.\"\"\"\n",
    "    tokens = tokenizer(sent)\n",
    "    tagged_tokens = nltk.pos_tag(tokens, tagset=\"universal\")\n",
    "    tagged_tokens = [(t, pos2wn.get(pos_tag, None))\n",
    "                     for (t, pos_tag) in tagged_tokens]\n",
    "    sentence_counts = {\"pos\": 0, \"neg\": 0}\n",
    "    for (t, pos_tag) in tagged_tokens:\n",
    "        token_label = memory.get(t, None)\n",
    "        if token_label is None:\n",
    "            token_label = \"neg\"\n",
    "            ss = lesk(tokens, t, pos=pos_tag)\n",
    "            if ss:\n",
    "                sense = swn.senti_synset(ss.name())\n",
    "                if sense.pos_score() >= sense.neg_score():\n",
    "                    token_label = \"pos\"\n",
    "            if update_mem:\n",
    "                memory[t] = token_label\n",
    "        sentence_counts[token_label] += 1\n",
    "    return sentence_counts\n",
    "\n",
    "\n",
    "def swn_sentence_classification(sent, tokenizer, memory, update_mem):\n",
    "    valence_counts = valence_count(sent, tokenizer, memory, update_mem)\n",
    "    return 0 if valence_counts[\"neg\"] > valence_counts[\"pos\"] else 1\n",
    "\n",
    "\n",
    "class DiffPosNegVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Class for implementing the DiffPosNeg feature as described in https://aclanthology.org/I13-1114/\n",
    "    through scikit-learn APIs.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer=word_tokenize, lb=0, ub=1):\n",
    "        \"\"\"\n",
    "        - :param tokenizer: Callable parameter, used to extract tokens from documents\n",
    "        when vectorizing;\n",
    "        - :param lb: lower bound for clipping absolute values of numerical distances once scaled;\n",
    "        - :param rb: same as :param lb:, but upper bound.\n",
    "        \"\"\"\n",
    "        super(BaseEstimator, self).__init__()\n",
    "        super(TransformerMixin, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "\n",
    "    def diff_pos_neg_feature(self, doc, memory, update_mem=False, as_ratio=True) -> list:\n",
    "        \"\"\"Returns the DiffPosNeg feature of :param: doc.\n",
    "        The feature is defined as the numerical distance between sentences\n",
    "        with a positive orientation and sentences with a negative orientation.\"\"\"\n",
    "        pos_count, neg_count = 0, 0\n",
    "        for sent in sent_tokenize(doc):\n",
    "            sent_cls = swn_sentence_classification(\n",
    "                sent, self.tokenizer, memory, update_mem)\n",
    "            if sent_cls == 0:\n",
    "                neg_count += 1\n",
    "            else:\n",
    "                pos_count += 1\n",
    "        if pos_count >= neg_count:\n",
    "            if as_ratio:\n",
    "                return [abs(pos_count-neg_count)/(pos_count+neg_count), 1]\n",
    "            return [abs(pos_count-neg_count), 1]\n",
    "        if as_ratio:\n",
    "            return [abs(pos_count-neg_count)/(pos_count+neg_count), 0]\n",
    "        return [abs(pos_count - neg_count), 0]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.memory_ = {}\n",
    "        # apply parallel execution of the 'diff_pos_neg' feature extraction function\n",
    "        with mp.Manager() as manager:\n",
    "            mem = manager.dict()\n",
    "            with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "                diff_pos_neg_feats = np.array(pool.starmap(\n",
    "                    self.diff_pos_neg_feature, [(doc, mem, True) for doc in X]))\n",
    "            self.memory_ = {k: v for k, v in mem.items()}\n",
    "        distances = diff_pos_neg_feats[:, 0]\n",
    "        self.min_ = np.amin(distances)\n",
    "        self.max_ = np.amax(distances)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        in_time = time.time()\n",
    "        # apply parallel execution of the 'diff_pos_neg' feature extraction function\n",
    "        with mp.Manager() as manager:\n",
    "            mem = manager.dict()\n",
    "            mem = {k: v for k, v in self.memory_.items()}\n",
    "            with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "                diff_pos_neg_feats = np.array(pool.starmap(\n",
    "                    self.diff_pos_neg_feature, [(doc, mem, False) for doc in X]))\n",
    "        distances = diff_pos_neg_feats[:, 0]\n",
    "        prevalences = diff_pos_neg_feats[:, -1]\n",
    "\n",
    "        # scale the values in the range [0,100], taking care of possible values outside the fitted min/max by clipping\n",
    "        distances = np.clip((distances - self.min_) / (self.max_ -\n",
    "                            self.min_ + np.finfo(float).eps), a_min=self.lb, a_max=self.ub)\n",
    "        distances = np.int16(distances*100)\n",
    "\n",
    "        # put components together and return\n",
    "        distances = np.expand_dims(distances, axis=-1)\n",
    "        prevalences = np.expand_dims(np.array(prevalences), axis=-1)\n",
    "        print(f\"Transformed {len(X)} documents in {time.time()-in_time:.2f}s\")\n",
    "        return hconcat(distances, prevalences)\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        in_time = time.time()\n",
    "        self.memory_ = {}\n",
    "        # apply parallel execution of the 'diff_pos_neg' feature extraction function\n",
    "        with mp.Manager() as manager:\n",
    "            mem = manager.dict()\n",
    "            with mp.Pool(processes=mp.cpu_count()) as pool:\n",
    "                diff_pos_neg_feats = np.array(pool.starmap(\n",
    "                    self.diff_pos_neg_feature, [(doc, mem, True) for doc in X]))\n",
    "            self.memory_ = {k: v for k, v in mem.items()}\n",
    "        distances = diff_pos_neg_feats[:, 0]\n",
    "        prevalences = diff_pos_neg_feats[:, -1]\n",
    "        print(\"Number of positive documents: {}\".format(\n",
    "            np.count_nonzero(prevalences)))\n",
    "\n",
    "        # override stats inferred from the data\n",
    "        self.min_ = np.amin(distances)\n",
    "        self.max_ = np.amax(distances)\n",
    "\n",
    "        # scaling the values of the distances in the range [0, 1]\n",
    "        distances = (distances - self.min_) / \\\n",
    "            (self.max_ - self.min_ + np.finfo(float).eps)\n",
    "        distances = np.int16(distances*100)\n",
    "\n",
    "        # put the feature components back together after post-processing and return\n",
    "        distances = np.expand_dims(distances, axis=-1)\n",
    "        prevalences = np.expand_dims(prevalences, axis=-1)\n",
    "        print(\n",
    "            f\"Fitted Model and transformed {len(X)} documents in {time.time()-in_time:.2f}s\")\n",
    "        return hconcat(distances, prevalences)\n",
    "\n",
    "\n",
    "def switch_vectorizer(vectorizer_name=\"count\"):     #---> utils.miscellaneous\n",
    "    assert vectorizer_name in (\"count\", \"tfidf\", \"diffposneg\", \"bert\")\n",
    "    if vectorizer_name == \"count\":\n",
    "        return sklearn.feature_extraction.text.CountVectorizer(tokenizer=word_tokenize)\n",
    "    elif vectorizer_name == \"tfidf\":\n",
    "        return sklearn.feature_extraction.text.TfidfVectorizer(tokenizer=word_tokenize)\n",
    "    elif vectorizer_name == \"diffposneg\":\n",
    "        return DiffPosNegVectorizer()\n",
    "\n",
    "def classification_report_csv(save_path, report):\n",
    "    '''Saves Classification Report as csv'''\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    for line in lines[2:-3]:\n",
    "        row = {}\n",
    "        row_data = line.split('      ')\n",
    "        row['class'] = row_data[0]\n",
    "        row['precision'] = float(row_data[1])\n",
    "        row['recall'] = float(row_data[2])\n",
    "        row['f1_score'] = float(row_data[3])\n",
    "        row['support'] = float(row_data[4])\n",
    "        report_data.append(row)\n",
    "    dataframe = pd.DataFrame.from_dict(report_data)\n",
    "    dataframe.to_csv(os.path.join(save_path, 'classification_report.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Testing Vectorizer and Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T18:18:42.484216Z",
     "iopub.status.busy": "2022-08-04T18:18:42.484128Z",
     "iopub.status.idle": "2022-08-04T18:18:42.485879Z",
     "shell.execute_reply": "2022-08-04T18:18:42.485643Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: wrap this up into a proper python file\n",
    "#representation = 'count'\n",
    "representation = 'tfidf'\n",
    "#classifier = 'multinomial'\n",
    "classifier = 'bernoulli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T18:18:42.487346Z",
     "iopub.status.busy": "2022-08-04T18:18:42.487264Z",
     "iopub.status.idle": "2022-08-04T18:18:42.488989Z",
     "shell.execute_reply": "2022-08-04T18:18:42.488716Z"
    }
   },
   "outputs": [],
   "source": [
    "assert representation in ('count', 'tfidf')\n",
    "assert classifier in ('multinomial', 'bernoulli')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Instantiate Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T18:18:42.490472Z",
     "iopub.status.busy": "2022-08-04T18:18:42.490389Z",
     "iopub.status.idle": "2022-08-04T18:18:42.492060Z",
     "shell.execute_reply": "2022-08-04T18:18:42.491835Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate vectorizer\n",
    "vectorizer = switch_vectorizer(representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Instantiate Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T18:18:42.493507Z",
     "iopub.status.busy": "2022-08-04T18:18:42.493432Z",
     "iopub.status.idle": "2022-08-04T18:18:42.495838Z",
     "shell.execute_reply": "2022-08-04T18:18:42.495584Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "\n",
    "# instantiate classifier\n",
    "if classifier == 'multinomial':\n",
    "    clf = MultinomialNB()\n",
    "else:\n",
    "    clf = BernoulliNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. Fit Vectorizer\n",
    "- At this step we first fit the vectorizer on the Movie Reviews dataset to link it to its vocab. All of this is done bearing in mind that *MovieReviews* will be the target dataset for the final evaluation!\n",
    "- Then we vectorize the RottenIMDB Dataset with the vocab constraints from just extracted from Movie Reviews dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T18:18:42.497389Z",
     "iopub.status.busy": "2022-08-04T18:18:42.497300Z",
     "iopub.status.idle": "2022-08-04T18:18:47.754634Z",
     "shell.execute_reply": "2022-08-04T18:18:47.754343Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/nlu/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# fit vectorizer on Movie Reviews\n",
    "vectorizer.fit(movie_reviews_ds)\n",
    "\n",
    "# vectorize the RottenIMDB Dataset with the vocab constraints from Movie Reviews\n",
    "vectors = vectorizer.transform(imdb_ds)\n",
    "labels = [1]*len(subj) + [0]*len(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4. N-Fold Cross Validation\n",
    "- At this step we perform cross validation and grab the best estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T18:18:47.756566Z",
     "iopub.status.busy": "2022-08-04T18:18:47.756443Z",
     "iopub.status.idle": "2022-08-04T18:18:49.039775Z",
     "shell.execute_reply": "2022-08-04T18:18:49.038792Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "scores = cross_validate(\n",
    "    estimator=clf, X=vectors, y=labels, \n",
    "    cv=StratifiedKFold(n_splits=10), \n",
    "    scoring=['f1_micro'],\n",
    "    return_estimator=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "estimator = scores['estimator'][np.argmax(np.array(scores['test_f1_micro']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T18:18:49.042944Z",
     "iopub.status.busy": "2022-08-04T18:18:49.042716Z",
     "iopub.status.idle": "2022-08-04T18:18:49.046017Z",
     "shell.execute_reply": "2022-08-04T18:18:49.045631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1 Score from cross validation: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Displaying cross validation results\n",
    "average = sum(scores['test_f1_micro'])/len(scores['test_f1_micro'])\n",
    "print(\"Average F1 Score from cross validation: {:.2f}\".format(average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5. Get Best Estimator\n",
    "Get best estimator on IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T18:18:49.047716Z",
     "iopub.status.busy": "2022-08-04T18:18:49.047616Z",
     "iopub.status.idle": "2022-08-04T18:18:49.058966Z",
     "shell.execute_reply": "2022-08-04T18:18:49.058689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.93      0.95      4916\n",
      "           1       0.93      0.98      0.95      4916\n",
      "\n",
      "    accuracy                           0.95      9832\n",
      "   macro avg       0.95      0.95      0.95      9832\n",
      "weighted avg       0.95      0.95      0.95      9832\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "y_pred = estimator.predict(vectors)\n",
    "report = classification_report(labels, y_pred)\n",
    "# classification_report('tmp/tuning/',report)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.6. Save Best Estimator and Vectorizer\n",
    "Both vectorizer and estimator are saved for future uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T18:18:49.060527Z",
     "iopub.status.busy": "2022-08-04T18:18:49.060437Z",
     "iopub.status.idle": "2022-08-04T18:18:49.210414Z",
     "shell.execute_reply": "2022-08-04T18:18:49.210148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at:  tmp/models/count_multinomial_subj_det_model.joblib\n",
      "Saving vectorizer at:  tmp/models/count_multinomial_subj_det_vectorizer.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tmp/models/count_multinomial_subj_det_vectorizer.joblib']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from joblib import dump\n",
    "\n",
    "# save estimator\n",
    "path_to_subj_detector = f'tmp/models/{representation}_{classifier}_subj_det_model.joblib'\n",
    "\n",
    "if not os.path.exists(os.path.dirname(path_to_subj_detector)):\n",
    "    os.makedirs(os.path.dirname(path_to_subj_detector))\n",
    "print(\"Saving model at: \", path_to_subj_detector)\n",
    "dump(estimator, path_to_subj_detector)\n",
    "\n",
    "\n",
    "# save vectorizer\n",
    "path_to_vectorizer = f'tmp/models/{representation}_{classifier}_subj_det_vectorizer.joblib'\n",
    "if not os.path.exists(os.path.dirname(path_to_vectorizer)):\n",
    "    os.makedirs(os.path.dirname(path_to_vectorizer))\n",
    "print(\"Saving vectorizer at: \", path_to_vectorizer)\n",
    "dump(vectorizer, path_to_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sets up `nltk` packages for [main](02-main.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d01cbc44f1dcf325f86e9e4d0791276c60c7cf84738fb5dcbe5f268c20cf97e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
