{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import nltk\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import dump\n",
    "from typing import Callable\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "# Local Imports\n",
    "from src.utils import mr2str, get_movie_reviews_dataset, vconcat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Vocabulary Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "TAGSET_LIST = ['VERB', 'NOUN', 'PRON', 'ADJ', 'ADV', 'ADP', 'CONJ', 'DET', 'NUM', 'PRT', 'X', '.']\n",
    "SUBJ_THRESH = 0.5\n",
    "\n",
    "# PATHS\n",
    "PATH_TO_IMDB = 'data/rotten_imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def freq_list(corpus: list[str], return_vocab: bool = False):\n",
    "    \"\"\"Builds the frequency list of a corpus. Returns a dictionary\n",
    "    where words are the keys and their frequency in the corpus is the respective value.\n",
    "\n",
    "    If :param return_vocab: is True, the vocabulary of the corpus is returned alongside\n",
    "    the frequency list.\"\"\"\n",
    "    out = {}\n",
    "    for sent in corpus:\n",
    "        tokens = word_tokenize(sent)\n",
    "        for token in tokens:\n",
    "            if token not in out.keys():\n",
    "                out[token] = 0\n",
    "            out[token] += 1\n",
    "    if return_vocab:\n",
    "        return out, set(list(out.keys()))\n",
    "    return out\n",
    "\n",
    "\n",
    "def doc_freq_list(corpus: list[str]) -> dict:\n",
    "    \"\"\"Builds the document-based frequency list of a corpus. Returns a dictionary\n",
    "    where words are the keys and their document-frequency in the corpus is the respective value.\"\"\"\n",
    "    out = {}\n",
    "    for i, sent in enumerate(corpus):\n",
    "        tokens = word_tokenize(sent)\n",
    "        for token in tokens:\n",
    "            if token not in out.keys():\n",
    "                out[token] = []\n",
    "            out[token].append(i)\n",
    "    for token in out.keys():\n",
    "        out[token] = len(set(out[token]))\n",
    "    return out\n",
    "\n",
    "\n",
    "def load_rotten_imdb(path):\n",
    "    subjective_sentences = \"quote.tok.gt9.5000\"\n",
    "    objective_sentences = \"plot.tok.gt9.5000\"\n",
    "\n",
    "    subj = []\n",
    "    with open(os.path.join(path, subjective_sentences), 'r') as f:\n",
    "        [subj.append(sent.strip()) for sent in f.readlines()]\n",
    "\n",
    "    obj = []\n",
    "    with open(os.path.join(path, objective_sentences), 'r') as f:\n",
    "        [obj.append(sent.strip()) for sent in f.readlines()]\n",
    "\n",
    "    return subj, obj\n",
    "\n",
    "\n",
    "def filter_dict(fl, vocab):\n",
    "    '''Removes entries from :param fl: frequency list not in :param vocab:.'''\n",
    "    out = {}\n",
    "    for k, v in fl.items():\n",
    "        if k in vocab:\n",
    "            out[k] = v\n",
    "    return out\n",
    "\n",
    "\n",
    "def tf(token, freq_list):\n",
    "    return freq_list[token]\n",
    "\n",
    "\n",
    "def idf(token, doc_freq_list, ndocs):\n",
    "    return math.log((1+ndocs)/ 1 + doc_freq_list[token] ) + 1\n",
    "\n",
    "\n",
    "def tfidf(token, freq_list, doc_freq_list, ndocs):\n",
    "    return tf(token, freq_list) * idf(token, doc_freq_list, ndocs)\n",
    "\n",
    "\n",
    "def normalize(vector):\n",
    "    \"\"\"Normalizes :param vector: with Euclidean Normalization.\"\"\"\n",
    "    denom = 0\n",
    "    for item in vector:\n",
    "        denom += item**2\n",
    "    denom = math.sqrt(denom)\n",
    "    for i in range(len(vector)):\n",
    "        vector[i] /= denom\n",
    "    return vector\n",
    "\n",
    "\n",
    "def tfidf_dict(freq_list, doc_freq_list, ndocs, norm=True):\n",
    "    \"\"\"Builds a dict with the tfidf (Term-Frequency - Inverse-Document-Frequency) value for each\n",
    "    word.  \n",
    "\n",
    "    - :param fl: frequency list of the vocabulary;\n",
    "    - :param dfl: document-based frequency list of the vocabulary;\n",
    "    - :ndocs: total number of documents in the corpus;\n",
    "    - :norm: decide whether to normalize the tfidf values using Euclidean Normalization or not.\n",
    "    \"\"\"\n",
    "    words = set(freq_list.keys())\n",
    "    out = {}\n",
    "    for word in words:\n",
    "        out[word] = tfidf(word, freq_list, doc_freq_list, ndocs)\n",
    "    if norm:\n",
    "        values = normalize(list(out.values()))\n",
    "        return dict(zip(words, values))\n",
    "    return out\n",
    "\n",
    "###################### EMBEDDING MATRIX ######################\n",
    "## Functions below are inspired by laboratories of the NLU  ##\n",
    "##  course, held by Evgeny A. Stepanov.                     ##\n",
    "##############################################################\n",
    "\n",
    "def position_features(sentence: str, tokenizer: Callable, vocab: set) -> list:\n",
    "    \"\"\"Encodes the relative position of the tokens in :param sentence: as follows:\n",
    "\n",
    "    - 0 for tokens at the beginning of the sentence; \n",
    "    - 1 for tokens in the middle of the sentence;\n",
    "    - 2 for tokens at the end of the sentence.\n",
    "\n",
    "    Tokens are extracted with the :param tokenizer: callable and filtered based on\n",
    "    whether they appear in :param vocab: or not.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    tokens = tokenizer(sentence)\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token not in vocab:\n",
    "            continue\n",
    "        if i == 0:\n",
    "            out.append(0)\n",
    "        elif i == len(tokens):\n",
    "            out.append(2)\n",
    "        else:\n",
    "            out.append(1)\n",
    "    return out\n",
    "\n",
    "\n",
    "def part_of_speech_features(sentence: str, tokenizer: Callable, vocab: set) -> list:\n",
    "    \"\"\"Encode the pos tags of the tokens in :param sentence: in a vectorial representation,\n",
    "    mapping the indices of the tags from the Universal Tagset. :param sentence: is split with the  :param tokenizer:\n",
    "    callable; resulting tokens not in :param vocab: are filtered out before applying the transformation.\"\"\"\n",
    "    tokens = tokenizer(sentence)\n",
    "    tagged_tokens = nltk.pos_tag(tokens, tagset=\"universal\")\n",
    "    ret = []\n",
    "    for tok, tag in tagged_tokens:\n",
    "        if tok in vocab:\n",
    "            ret.append(TAGSET_LIST.index(tag))\n",
    "    return ret\n",
    "\n",
    "\n",
    "def negation_feature(sent: str, tokenizer: Callable, vocab: set) -> list:\n",
    "    \"\"\"Encodes :param sent: extracting the Negation Feature. The Negation Feature is\n",
    "    defined as a vector where a 1 indicates a token being part of a negated phrase and 0 viceversa.\n",
    "\n",
    "    Tokens are extracted with the :param tokenizer: callable and filtered out based on their appearance\n",
    "    in :param vocab: before transforming the sentence.\"\"\"\n",
    "    tokens = tokenizer(sent)\n",
    "    valid_tokens = []\n",
    "    for t in tokens:\n",
    "        if t in vocab:\n",
    "            valid_tokens.append(t)\n",
    "    marked_sent = mark_negation(valid_tokens)\n",
    "    return [1 if t.endswith(\"_NEG\") else 0 for t in marked_sent]\n",
    "\n",
    "\n",
    "def embed_sentence(sent, tokenizer, vocabulary, tfidf_map):\n",
    "    \"\"\"Encodes a sentence extracting  a subset of token-level features \n",
    "    w.r.t. the ones proposed in https://arxiv.org/pdf/1312.6962.pdf.\n",
    "\n",
    "    The features for each token (extracted with the :param tokenizer: callable) of :param sent: are:  \n",
    "    - its tfidf feature (using :param tfidf_map:);\n",
    "    - its positional feature;\n",
    "    - its part_of_speech feature:\n",
    "    - its negation feature.\n",
    "\n",
    "    Thus, a matrix of shape (N_tokens, 4) is returned.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer(sent)\n",
    "    tfidf_feats = []\n",
    "    position_feats = position_features(sent, tokenizer, vocabulary)\n",
    "    part_of_speech_feats = part_of_speech_features(sent, tokenizer, vocabulary)\n",
    "    negation_feats = negation_feature(sent, tokenizer, vocabulary)\n",
    "    for token in tokens:\n",
    "        if token in vocabulary:\n",
    "            tfidf_feats.append(tfidf_map.get(token))\n",
    "\n",
    "    tfidf_feats = np.expand_dims(np.array(tfidf_feats), axis=-1)\n",
    "    position_feats = np.expand_dims(np.array(position_feats), axis=-1)\n",
    "    part_of_speech_feats = np.expand_dims(\n",
    "        np.array(part_of_speech_feats), axis=-1)\n",
    "    negation_feats = np.expand_dims(np.array(negation_feats), axis=-1)\n",
    "\n",
    "    X = np.concatenate((tfidf_feats, part_of_speech_feats,\n",
    "                       position_feats, negation_feats), axis=1)\n",
    "    return X\n",
    "\n",
    "\n",
    "def token_count(ds, tokenizer, vocab):\n",
    "    count = 0\n",
    "    for sent in ds:\n",
    "        tokens = tokenizer(sent)\n",
    "        for t in tokens:\n",
    "            if t in vocab:\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "### CLASSIFICATION ###\n",
    "def classify_sentence(clf, sent, subjectivity_thresh, tokenizer, vocabulary, tfidf_map):\n",
    "    \"\"\"Performs token-level subjectivity detection on the tokens in :param sent:, then aggregates\n",
    "    the results for sentence-level classification. If the percentage of subjective tokens exceeds\n",
    "    :param subjectivity_thresh:, then :param sent: is classified as subjective (objective otherwise).\"\"\"\n",
    "    X = embed_sentence(sent, tokenizer, vocabulary, tfidf_map)\n",
    "    y = clf.predict(X)\n",
    "    if np.count_nonzero(y) >= int(len(y)*subjectivity_thresh):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Movie Reviews Dataset\n",
    "Here we Load Movie Reviews Dataset and perform vocabulary extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, neg = get_movie_reviews_dataset(mark_negs=False)\n",
    "pos = mr2str(pos)\n",
    "neg = mr2str(neg)\n",
    "movie_reviews_ds = neg + pos\n",
    "_, vocab = freq_list(movie_reviews_ds, return_vocab=True)\n",
    "#len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Rotten IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj, obj = load_rotten_imdb(PATH_TO_IMDB)\n",
    "imdb_ds = subj + obj\n",
    "#imdb_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Unigram Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqency_list = filter_dict(freq_list(imdb_ds), vocab)\n",
    "document_freq_list = filter_dict(doc_freq_list(imdb_ds), vocab)\n",
    "n_docs = len(movie_reviews_ds)\n",
    "tfidf_map = tfidf_dict(freqency_list, document_freq_list, n_docs)\n",
    "#tfidf_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = None\n",
    "for sent in tqdm(imdb_ds):\n",
    "    embedded_sent_matrix = embed_sentence( sent, word_tokenize, vocab, tfidf_map )\n",
    "    if X is None:\n",
    "        X = embedded_sent_matrix\n",
    "    else:\n",
    "        X = vconcat(X, embedded_sent_matrix)\n",
    "labels = [1] * token_count(subj, word_tokenize, vocab) + [0] * token_count(obj, word_tokenize, vocab)\n",
    "#len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. N-Fold Cross Validation\n",
    "- At this step we perform cross validation and grab the best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "scores = cross_validate(\n",
    "    clf, X, labels,\n",
    "    cv = StratifiedKFold(n_splits=10),\n",
    "    scoring = ['f1_micro'],\n",
    "    return_estimator = True,\n",
    "    n_jobs = -1)\n",
    "\n",
    "average = sum(scores['test_f1_micro']) / len(scores['test_f1_micro'])\n",
    "print(\"F1 Score: {:.3f}\".format(average))\n",
    "\n",
    "estimator = scores['test_f1_micro'][np.argmax(np.array(scores['test_f1_micro']))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Getting predictions using best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1]*len(subj) + [0]*len(obj)\n",
    "y_hat = [classify_sentence(estimator, sent, SUBJ_THRESH,\n",
    "                                word_tokenize, vocab, tfidf_map) for sent in imdb_ds]\n",
    "print(classification_report(y, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- save best model to models folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save estimator\n",
    "path_to_estimator = f'tmp/models/token_level_subj_det.joblib'\n",
    "if not os.path.exists(os.path.dirname(path_to_estimator)):\n",
    "    os.makedirs(os.path.dirname(path_to_estimator))\n",
    "print(\"Saving estimator at: \", path_to_estimator)\n",
    "dump(estimator, path_to_estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nlu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d01cbc44f1dcf325f86e9e4d0791276c60c7cf84738fb5dcbe5f268c20cf97e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
